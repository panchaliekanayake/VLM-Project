# -*- coding: utf-8 -*-
"""StructuredAnswers

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nkL9YL5c8IN1XQenCUbEuZ6a7a_GIdqA
"""

# ======================
# 1. Install dependencies
# ======================


# ======================
# 2. Imports
# ======================
import pandas as pd
import torch
from transformers import pipeline
import re
from sklearn.metrics import r2_score
from tqdm import tqdm

# ======================
# 4. Load Data
# ======================
transcripts_path = "/home/labuser/research/VLM-Project/data/Transcripts_All.xlsx"
scores_path = "/home/labuser/research/VLM-Project/data/Transcripts_All.xlsx"

transcripts_df = pd.read_excel(transcripts_path)
scores_df = pd.read_excel(scores_path)

print("Transcript columns:", transcripts_df.columns)
print("Score columns:", scores_df.columns)

# ======================
# 5. Setup Hugging Face Model
# ======================
model_name = "google/flan-t5-large"
device = 0 if torch.cuda.is_available() else -1

generator = pipeline(
    "text2text-generation",
    model=model_name,
    device=device,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
)

# ======================
# 6. Helper function: chunk transcript
# ======================
def chunk_transcript(transcript, max_words=80):
    """
    Split transcript into chunks of ~max_words each.
    Uses | separator to split by speaker turns.
    """
    parts = transcript.split("|")
    chunks = []
    current_chunk = ""
    current_count = 0

    for part in parts:
        words = part.split()
        if current_count + len(words) > max_words:
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            current_chunk = part
            current_count = len(words)
        else:
            current_chunk += " " + part
            current_count += len(words)

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks

# ======================
# 7. Helper function: predict score for a chunk
# ======================
def predict_chunk_score(chunk):
    prompt = f"""
You are an expert communication evaluator.
Analyze the following interview transcript and score it from 0 to 9 for StructuredAnswers.
Return ONLY a number (can be decimal).

Transcript:
{chunk}
"""
    try:
        output = generator(prompt, max_new_tokens=50, do_sample=False)[0]["generated_text"]
        # Extract first valid number <=9
        matches = re.findall(r"\d+(?:\.\d+)?", output)
        if matches:
            scores = [float(m) for m in matches if m.strip() != "" and float(m) <= 9]
            return scores[0] if scores else None
        return None
    except Exception as e:
        print("Error:", e)
        return None

# ======================
# 8. Run Inference for all participants
# ======================
results = []

for i, row in tqdm(transcripts_df.iterrows(), total=len(transcripts_df)):
    participant = row["Participant"]
    transcript = row["Transcripts"]

    # 1. Chunk transcript
    chunks = chunk_transcript(transcript, max_words=80)

    # 2. Predict each chunk
    chunk_scores = [predict_chunk_score(c) for c in chunks]

    # 3. Aggregate scores (average)
    valid_scores = [s for s in chunk_scores if s is not None]
    final_score = sum(valid_scores) / len(valid_scores) if valid_scores else None

    results.append({
        "Participant": participant,
        "Predicted_StructuredAnswers": final_score
    })

pred_df = pd.DataFrame(results)

# ======================
# 9. Merge with actual scores
# ======================
# Assuming the scores are in the 'Transcripts' column of scores_df
scores_df = scores_df.rename(columns={'Transcripts': 'StructuredAnswers'})

final_df = pred_df.merge(
    scores_df[["Participant", "StructuredAnswers"]],
    on="Participant",
    how="left"
)

# ======================
# 10. Show Predicted vs Actual for first 20 participants
# ======================
print("\nFirst 20 results:")
print(final_df[["Participant", "Predicted_StructuredAnswers", "StructuredAnswers"]].head(20))

# ======================
# 11. Show summary statistics
# ======================
print("\nSummary of Predicted Scores:")
print(final_df["Predicted_StructuredAnswers"].describe())

# Save results to CSV
output_file = "predictions_results.csv"
final_df.to_csv(output_file, index=False)
print(f"\nResults saved to {output_file}")

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load your transcripts and scores
transcripts_df = pd.read_excel("/home/labuser/research/VLM-Project/data/Transcripts_All.xlsx")
scores_df = pd.read_excel("/home/labuser/research/VLM-Project/data/All_Scores.xlsx")

# Suppose you already got LLM predictions as a list of dicts
# Include participant IDs and predictions
# Example: results = [{"Participant": "p1", "Predicted_StructuredAnswers": 4.625}, ...]

results_df = pd.DataFrame(results)

# Merge with true scores
results_df = results_df.merge(scores_df[["Participant", "StructuredAnswers"]],
                              on="Participant",
                              how="left")

# Now columns exist
X = results_df[["Predicted_StructuredAnswers"]].values
y = results_df["StructuredAnswers"].values

# Fit linear regression
reg = LinearRegression()
reg.fit(X, y)

# Adjusted predictions
results_df["Adjusted_Prediction"] = reg.predict(X)

# Evaluate
r2 = r2_score(y, results_df["Adjusted_Prediction"])
mse = mean_squared_error(y, results_df["Adjusted_Prediction"])

print("Linear Regression Mapping:")
print(f"Coefficient: {reg.coef_[0]:.4f}, Intercept: {reg.intercept_:.4f}")
print(f"RÂ² Score: {r2:.4f}, MSE: {mse:.4f}")

# Show first 20 candidates
print(results_df.head(20)[["Participant", "Predicted_StructuredAnswers",
                            "Adjusted_Prediction", "StructuredAnswers"]])
