# -*- coding: utf-8 -*-
"""StructuredAnswers

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nkL9YL5c8IN1XQenCUbEuZ6a7a_GIdqA
"""

# ======================
# Imports
# ======================
import pandas as pd
import torch
from transformers import pipeline
import re
from sklearn.metrics import r2_score
from tqdm import tqdm

# ======================
# Load Data
# ======================
transcripts_path = "/home/labuser/research/VLM-Project/data/Transcripts_All.xlsx"
scores_path = "/home/labuser/research/VLM-Project/data/Transcripts_All.xlsx"

transcripts_df = pd.read_excel(transcripts_path)
scores_df = pd.read_excel(scores_path)

print("Transcript columns:", transcripts_df.columns)
print("Score columns:", scores_df.columns)

# ======================
# 5. Setup Hugging Face Model
# ======================
model_name = "google/flan-t5-large"
device = 0 if torch.cuda.is_available() else -1

generator = pipeline(
    "text2text-generation",
    model=model_name,
    device=device,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
)

# ======================
# Helper function: chunk transcript
# ======================
def chunk_transcript(transcript, max_words=80):
    """
    Split transcript into chunks of ~max_words each.
    Uses | separator to split by speaker turns.
    """
    parts = transcript.split("|")
    chunks = []
    current_chunk = ""
    current_count = 0

    for part in parts:
        words = part.split()
        if current_count + len(words) > max_words:
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            current_chunk = part
            current_count = len(words)
        else:
            current_chunk += " " + part
            current_count += len(words)

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks

# ======================
# Helper function: predict score for a chunk
# ======================
def predict_chunk_score(chunk):
    prompt = f"""
You are an expert communication evaluator.

TASK
Evaluate the STRUCTURE of answers in the following interview transcript between an interviewer and an interviewee.
The transcript contains multiple question–answer pairs labeled as:
Interviewer: ...
Interviewee: ...

DEFINITION — "StructuredAnswers"
How clearly and logically the answer is organized and presented:
- Clear opening or framing statement.
- Logical flow of ideas (point → reason → example → conclusion).
- Smooth transitions or signposting (e.g., “first”, “because”, “for example”).
- Grouping of related ideas and avoidance of tangents.
- Concluding or wrap-up sentence that connects back to the question.

SCORING SCALE (0–9)
9   = Excellent: well-organized, clear logic, smooth transitions, strong close.
7–8 = Good: mostly clear and logical; minor structural issues.
5–6 = Fair: understandable but loosely structured; weak start or ending.
3–4 = Weak: scattered or unclear sequence; little structure.
1–2 = Poor: rambling or disorganized; hard to follow.
0   = Not scorable or off-topic.

INSTRUCTIONS
1. Identify each interviewer question and the interviewee’s direct answer(s).
2. Assign a STRUCTURED ANSWERS score (0–9, decimals allowed) for each answer.
3. At the end, compute the **average** of all these scores.
4. Output the result in the following exact format:

Question 1: <short summary of question>
Score: <number>

Question 2: <short summary of question>
Score: <number>

...

Average StructuredAnswers Score: <average_number>

Return ONLY this structured list and average. No explanations or commentary.

Transcript:
{chunk}
"""
    return prompt

    try:
        output = generator(prompt, max_new_tokens=50, do_sample=False)[0]["generated_text"]
        # Extract first valid number <=9
        matches = re.findall(r"\d+(?:\.\d+)?", output)
        if matches:
            scores = [float(m) for m in matches if m.strip() != "" and float(m) <= 9]
            return scores[0] if scores else None
        return None
    except Exception as e:
        print("Error:", e)
        return None

# ======================
# Run Inference for all participants
# ======================
results = []

for i, row in tqdm(transcripts_df.iterrows(), total=len(transcripts_df)):
    participant = row["Participant"]
    transcript = row["Transcripts"]

    # 1. Chunk transcript
    chunks = chunk_transcript(transcript, max_words=80)

    # 2. Predict each chunk
    chunk_scores = [predict_chunk_score(c) for c in chunks]

    # 3. Aggregate scores (average)
    valid_scores = [s for s in chunk_scores if s is not None]
    final_score = sum(valid_scores) / len(valid_scores) if valid_scores else None

    results.append({
        "Participant": participant,
        "Predicted_StructuredAnswers": final_score
    })

pred_df = pd.DataFrame(results)

# ======================
# Merge with actual scores
# ======================
# Assuming the scores are in the 'Transcripts' column of scores_df
scores_df = scores_df.rename(columns={'Transcripts': 'StructuredAnswers'})

final_df = pred_df.merge(
    scores_df[["Participant", "StructuredAnswers"]],
    on="Participant",
    how="left"
)

# ======================
# Show Predicted vs Actual for first 20 participants
# ======================
print("\nFirst 20 results:")
print(final_df[["Participant", "Predicted_StructuredAnswers", "StructuredAnswers"]].head(20))

# ======================
# Show summary statistics
# ======================
print("\nSummary of Predicted Scores:")
print(final_df["Predicted_StructuredAnswers"].describe())

# Save results to CSV
output_file = "predictions_results.csv"
final_df.to_csv(output_file, index=False)
print(f"\nResults saved to {output_file}")

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load your transcripts and scores
transcripts_df = pd.read_excel("/home/labuser/research/VLM-Project/data/Transcripts_All.xlsx")
scores_df = pd.read_excel("/home/labuser/research/VLM-Project/data/All_Scores.xlsx")

# Suppose you already got LLM predictions as a list of dicts
# Include participant IDs and predictions
# Example: results = [{"Participant": "p1", "Predicted_StructuredAnswers": 4.625}, ...]

results_df = pd.DataFrame(results)

# Merge with true scores
results_df = results_df.merge(scores_df[["Participant", "StructuredAnswers"]],
                              on="Participant",
                              how="left")

# Now columns exist
X = results_df[["Predicted_StructuredAnswers"]].values
y = results_df["StructuredAnswers"].values

# Fit linear regression
reg = LinearRegression()
reg.fit(X, y)

# Adjusted predictions
results_df["Adjusted_Prediction"] = reg.predict(X)

# Evaluate
r2 = r2_score(y, results_df["Adjusted_Prediction"])
mse = mean_squared_error(y, results_df["Adjusted_Prediction"])

print("Linear Regression Mapping:")
print(f"Coefficient: {reg.coef_[0]:.4f}, Intercept: {reg.intercept_:.4f}")
print(f"R² Score: {r2:.4f}, MSE: {mse:.4f}")

# Show first 20 candidates
print(results_df.head(20)[["Participant", "Predicted_StructuredAnswers",
                            "Adjusted_Prediction", "StructuredAnswers"]])


# Save final results
results_df.to_csv("final_results.csv", index=False)