{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-10-04T15:04:00.384Z",
     "iopub.execute_input": "2025-10-04T15:03:18.164231Z",
     "iopub.status.busy": "2025-10-04T15:03:18.163519Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5452a7a1937c443a8aadb459eefd9c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] GPU load failed, trying CPU. Reason: CUDA out of memory. Tried to allocate 188.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 32.12 MiB is free. Process 4588 has 14.71 GiB memory in use. Of the allocated memory 14.33 GiB is allocated by PyTorch, and 257.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4af5c8e83da4904aecfdbb8b1102ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ======= simple_focused_inference.py =======\n",
    "import re, json, numpy as np, pandas as pd, torch\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ---- file paths ----\n",
    "TRANSCRIPTS_PATH = \"/home/labuser/research/VLM-Project/data/Transcripts_All.xlsx\"\n",
    "SCORES_PATH      = \"/home/labuser/research/VLM-Project/data/All_Scores.xlsx\"\n",
    "OUT_PATH         = \"/home/labuser/research/VLM-Project/data/focused_results.csv\"\n",
    "\n",
    "# ---- load data ----\n",
    "transcripts_df = pd.read_excel(TRANSCRIPTS_PATH)\n",
    "scores_df      = pd.read_excel(SCORES_PATH)\n",
    "\n",
    "# ---- ensure expected columns ----\n",
    "def norm_col(df, want):\n",
    "    if want in df.columns: return\n",
    "    for c in df.columns:\n",
    "        if c.strip().lower() == want.lower():\n",
    "            df.rename(columns={c: want}, inplace=True); return\n",
    "    raise AssertionError(f\"Missing '{want}' column.\")\n",
    "norm_col(transcripts_df, \"Participant\")\n",
    "norm_col(transcripts_df, \"Transcripts\")\n",
    "\n",
    "# ---- load model ----\n",
    "print(\"Loading model...\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pad_id = getattr(tokenizer, \"pad_token_id\", getattr(tokenizer, \"eos_token_id\", None))\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    device=device,\n",
    "    torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "    max_new_tokens=16,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    pad_token_id=pad_id,\n",
    "    return_full_text=False\n",
    ")\n",
    "print(f\"✅ Model loaded: {model_name}\")\n",
    "\n",
    "# ---- prompt for Focused ----\n",
    "FOCUSED_PROMPT = \"\"\"You are an evaluator.\n",
    "\n",
    "Give ONE overall score for the candidate’s **Focused** quality (0–9, decimals allowed).\n",
    "Definition:\n",
    "- Stays on-topic and directly answers questions.\n",
    "- Clear, organized, and logical.\n",
    "- Uses relevant details; avoids unrelated tangents.\n",
    "- Ignore pleasantries or filler words.\n",
    "\n",
    "0 = completely unfocused, 5 = somewhat focused, 9 = consistently direct and organized.\n",
    "Return only a number between 0 and 9.\n",
    "\n",
    "Transcript:\n",
    "\"\"\"\n",
    "\n",
    "# ---- helper to extract number ----\n",
    "def extract_score(text):\n",
    "    nums = re.findall(r'[-+]?\\d*\\.?\\d+', str(text))\n",
    "    for n in nums:\n",
    "        try:\n",
    "            v = float(n)\n",
    "            if 0.0 <= v <= 9.0:\n",
    "                return v\n",
    "        except:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# ---- scoring loop ----\n",
    "preds = []\n",
    "for _, row in tqdm(transcripts_df.iterrows(), total=len(transcripts_df)):\n",
    "    transcript = str(row[\"Transcripts\"])\n",
    "    prompt = FOCUSED_PROMPT + transcript + \"\\n\\nScore:\"\n",
    "    out = generator(prompt)\n",
    "    txt = out[0][\"generated_text\"] if isinstance(out, list) and \"generated_text\" in out[0] else str(out)\n",
    "    score = extract_score(txt)\n",
    "    preds.append({\"Participant\": row[\"Participant\"], \"Predicted_Focused\": score})\n",
    "\n",
    "pred_df = pd.DataFrame(preds)\n",
    "\n",
    "# ---- merge + evaluate ----\n",
    "final_df = pred_df.merge(scores_df, on=\"Participant\", how=\"left\")\n",
    "if \"Focused\" in final_df.columns:\n",
    "    y_true = pd.to_numeric(final_df[\"Focused\"], errors=\"coerce\")\n",
    "    y_pred = pd.to_numeric(final_df[\"Predicted_Focused\"], errors=\"coerce\")\n",
    "    mask = y_true.notna() & y_pred.notna()\n",
    "    r2  = r2_score(y_true[mask], y_pred[mask]) if mask.any() else np.nan\n",
    "    mse = mean_squared_error(y_true[mask], y_pred[mask]) if mask.any() else np.nan\n",
    "    rho = spearmanr(y_true[mask], y_pred[mask])[0] if mask.sum() >= 3 else np.nan\n",
    "    print(f\"\\nR²: {r2:.4f}\" if r2==r2 else \"R²: n/a\")\n",
    "    print(f\"MSE: {mse:.4f}\" if mse==mse else \"MSE: n/a\")\n",
    "    print(f\"Spearman ρ: {rho:.4f}\" if rho==rho else \"Spearman ρ: n/a\")\n",
    "else:\n",
    "    print(\"\\n⚠️ 'Focused' column not found; only predictions generated.\")\n",
    "\n",
    "# ---- save ----\n",
    "final_df.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\n✅ Results saved to: {OUT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8133846,
     "sourceId": 13123479,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
